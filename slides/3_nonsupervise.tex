\begin{frame}{3 L'apprentissage non-suppervisé}
  \begin{itemize}
  \item Données non \textit{labélisées}
  \item La machine apprend par elle même à indentifier une structure
  \item Évaluation des performances compliqué.
  \item Problèmes de classification, réduction de dimensions
  \item \textit{K-means}, \textit{Mean Shift}, \textit{Gaussian Mixture Model}
  \item Analyse en Composante Principale
  \end{itemize}  
\end{frame}

\begin{frame}{3.1 Les algorithmes de clustering}
  \begin{itemize}
  \item \textit{Clustering}: Se rapproche d'un problème de classification (sans labels)
  \item Ces algorithmes cherchent à rassembler les exemples en cluster
  \item À la différence des arbres de décision, le choix du cluster ne s'effectue pas par une suite de décisions simple, mais en déterminant la plus petite distance possible dans l'espace des variables
  \item Utilisés pour la segmentation d'utilisateurs/marchés dans le commerce en ligne mais aussi en génétique
  \item Il faut (dans la majorité des cas) définir au préalable le nombre de clusters à construire (hyperparamètre du modèle)
  \end{itemize}
\end{frame}

\begin{frame}{3.1 Les algorithmes de clustering}
  \begin{figure}
    \includegraphics[height=0.8\textheight]{figs/clusteringComparison.png}
  \end{figure}
\end{frame}

\begin{frame}{3.1 K-Means: description et limites}
  \begin{itemize}
  \item Sépare les données en $K$ clusters $C_{k}$ d'égale variance (dispersion)
  \item L'algorithme modifie la position des \textit{centroïdes} $\mu_{k}$ afin de minimiser l'écart moyen de l'ensemble des points d'un cluster à son \textit{centroïde}
  \end{itemize}
  \vspace{-0.5cm}
  \begin{minipage}{0.55\textwidth}
    \begin{itemize}
    \item Critère de minimisation, \textbf{Inertie:}
    \end{itemize}
    \begin{equation*}
      I = \displaystyle\sum_{k=0}^{K}\displaystyle\sum_{x \in C_{k}}\|x-\mu_{k}\|^{2}
    \end{equation*}
    \begin{center}
      \textbf{\textcolor{orange}{(Distance Euclidienne)}}
    \end{center}
  \end{minipage}
  \hfill
  \begin{minipage}{0.4\textwidth}
    \animategraphics[autoplay,loop,width=0.8\textwidth]{5}{figs/gif/kmeanAnim-}{0}{12} %% Insert gif as png list
  \end{minipage}
  \vspace{-1cm}
  \begin{itemize}
  \item \textbf{\textcolor{orange}{Limitations:}}
    \begin{itemize}
      \normalsize
    \item Le nombre de clusters se définit ``\textit{à la main}''
    \item Le résultat est très dépendant de l'initialisation
    \item Le \textbf{K-Means} ne fonctionne pas avec les variables catégorielles (non ordonnées)
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{3.1 K-Prototypes}
  \begin{itemize}
  \item Implémentation python: \underline{\href{https://github.com/nicodv/kmodes}{https://github.com/nicodv/kmodes}}
  \item Permet de mélanger variables \textcolor{orangeAgaetis}{numériques} (\boldmath $r$ \unboldmath) et \textcolor{orangeAgaetis}{catégorielles} (\boldmath $c$ \unboldmath)
  \item \textit{centroïdes:} $\mu_{k}$ \boldmath $\rightarrow$ \unboldmath \textit{prototypes:} $Q_{k}$
  \item On redéfinit la fonction de coût:
    \begin{equation*}
      I = \displaystyle\sum_{k=0}^{K}\displaystyle\sum_{x \in C_{k}} \Big( \displaystyle\sum_{j=1}^{m_{r}}(x_{j}^{r} - q_{jk}^{r})^{2} + \gamma_{k}\displaystyle\sum_{j=1}^{m_{c}}\delta(x_{j}^{c}, q_{jk}^{c})\Big) ~~\text{with}~~ \delta(p,q) = \begin{cases} 0 & \quad \text{if } p = q\\ 1 & \quad \text{if } p \neq q \end{cases}
    \end{equation*}
  \item \textbf{\textcolor{orangeAgaetis}{Algorithme:}}
    \begin{enumerate}
      \normalsize
    \item Initialise \boldmath $K$ \unboldmath prototypes (aléatoirement)
    \item Détermine le plus proche prototype de chaque entrée, et crée les clusters
    \item Met à jour les prototypes de chaque cluster en minimisant $I$
    \end{enumerate}
  \end{itemize}
\end{frame}

\begin{frame}{3.1 Clustering: Iris dataset, K-means result}
  \begin{figure}
    \includegraphics[width=0.7\textwidth]{figs/clusteringTrue.png}
    \includegraphics[width=0.7\textwidth]{figs/clusteringKmeans.png}
  \end{figure}
\end{frame}

\begin{frame}{3.1 Mean Shift}
  \begin{itemize}
  \item Cherche les zones de fortes densité en modifiant itérativement la position de \textit{centroïdes}
  \item Des \textit{centroïdes} de rayons $R$ sont aléatoirement initialisés
  \item Ils sont déplacés vers la région de plus haute densité (nombres de points dans le rayon $R$)
  \item On continue jusqu'à maximiser la densité de chaque \textit{centroïde}
  \item Plusieurs \textit{centroïdes} dans une zone: celui avec la plus haute densité est conservé
  \item l'ensemble du dataset est labelisé (plus petite distance)
  \item Le \textit{Mean Shift} détermine le nombre optimal de clusters pour la valeur du rayon choisie (R)
  \end{itemize}
\end{frame}

\begin{frame}{3.1 Clustering: Iris dataset, Mean Shift result}
  \begin{figure}
    \includegraphics[width=0.7\textwidth]{figs/clusteringTrue.png}
    \includegraphics[width=0.7\textwidth]{figs/clusteringMeanShift.png}
  \end{figure}
\end{frame}

\begin{frame}{3.1 Gaussian mixture models}
  \begin{itemize}
  \item \textbf{Hypothèse:} la structure des données est compatible avec un mélange de gaussiennes
  \end{itemize}
  \begin{minipage}{0.48\textwidth}
    \begin{itemize}
    \item On ajuste les paramètres de $N$ gaussiennes jusqu'à trouver ceux qui \textit{collent} le mieux aux données
    \end{itemize}
  \end{minipage}
  \begin{minipage}{0.48\textwidth}
    \begin{figure}
      \includegraphics[width=0.8\textwidth]{figs/gaussianFct.png}
    \end{figure}
    \tiny
    \vspace{-0.5cm}
    \begin{center}
      depuis \href{https://en.wikipedia.org/wiki/Gaussian_function}{\color{blue}{Wikipedia}}
    \end{center}
  \end{minipage}
  \begin{itemize}
  \item Algorithme d'ajustement: \textbf{espèrance-maximisation}, on cherche à maximiser la fonction de \textit{log-likelihood} (fonction de vraissemblance)
  \item \textit{Intuitivement:} on cherche les paramètres qui rendent la distribution de données observée la plus probable
  \end{itemize}
\end{frame}

\begin{frame}{3.1 Clustering: Iris dataset, GMM result}
  \begin{figure}
    \includegraphics[width=0.7\textwidth]{figs/clusteringTrue.png}
    \includegraphics[width=0.7\textwidth]{figs/clusteringGMM.png}
  \end{figure}
\end{frame}

\begin{frame}{3.2 Analyse en composantes principales}
  \begin{itemize}
  \item \textbf{PCA} (\textit{Composants principal analysis}): Algorithme de réduction de dimensions
  \item Transforme un problème à $n$ variables en un problème à $n'$ variables (avec $n' < n$)
  \item Explore les corrélations entre les variables pour les \textit{regrouper} (pondération): \textbf{Pertes d'informations}
  \item Très utile pour:
    \begin{itemize}
      \normalsize
    \item Pre-stage d'un algorithme de clustering (peut améliorer les performances)
    \item Pour faire de la visualisation en 2D de données à plus haute dimension
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{3.2 Analyse en composantes principales}
  \begin{itemize}
  \item \textbf{Exemple d'utilisation:} digit dataset:
    \begin{itemize}
      \normalsize
    \item 64 variables $\Rightarrow$ 10 pour initialiser le K-means
    \item 64 $\Rightarrow$ 2 pour la visualisation
    \end{itemize}
  \end{itemize}
  \begin{figure}
    \includegraphics[width=0.6\textwidth]{figs/PCA.png}
  \end{figure}
\end{frame}
