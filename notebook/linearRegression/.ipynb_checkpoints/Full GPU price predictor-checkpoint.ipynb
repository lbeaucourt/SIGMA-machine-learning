{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "\n",
    "# Fonction pour générer la droite représentant notre modèle\n",
    "def getHypothesisForPLot(theta):\n",
    "    return pd.DataFrame({'x':np.arange(0, 12000, 100)/1000,\n",
    "                         'y':[hypothesis(x,theta)/1000 for x in np.arange(0, 12000, 100)]})\n",
    "\n",
    "# on plot les données avec l'hypothèse correpondant à la valeur de theta \n",
    "#    ainsi que l'évolution dans la courbe de J(theta) en fonction de theta\n",
    "# On rajoute également la valeur de J(theta) en fonction du temps qui va nous servir à \n",
    "#   débuger notre algorithme\n",
    "\n",
    "def plotData(ax,x,y,theta,yhat,gradDescentEvol):\n",
    "    ax.plot(x,y,'o',label='data')\n",
    "    ax.plot(getHypothesisForPLot(theta).x,getHypothesisForPLot(theta).y,'r',label='hypothèse')\n",
    "    for i in range(x.shape[0]):\n",
    "        ax.plot((x[i],x[i]), (min(y[i],yhat[i]),max(y[i],yhat[i])), 'k-')\n",
    "    ax.legend(fontsize=12)\n",
    "\n",
    "def plotCostFunction(ax,x,y,theta,gradDescentEvol,thetaInit):\n",
    "#    thetaRange = np.arange(abs(thetaInit)-100,abs(thetaInit)+100,0.1)\n",
    "    thetaRange = np.arange(80-100,80+100,0.1)\n",
    "    costFctEvol = pd.DataFrame({'theta':thetaRange,\n",
    "                                'cost':[costFunction(y,hypothesis(x,genTheta))\n",
    "                                        for genTheta in thetaRange]})\n",
    "\n",
    "    ax.plot(costFctEvol.theta,costFctEvol.cost,label='J(theta)')\n",
    "    for i in range(gradDescentEvol.shape[0]):\n",
    "        ax.plot(gradDescentEvol.theta[i],gradDescentEvol.J[i],'ro')\n",
    "    for i in range(gradDescentEvol.shape[0]-1):\n",
    "        ax.plot((gradDescentEvol.theta[i],gradDescentEvol.theta[i+1]),\n",
    "                (gradDescentEvol.J[i],gradDescentEvol.J[i+1]),'k-',lw=1)\n",
    "    ax.legend(fontsize=12)\n",
    "\n",
    "def plotCostFunctionEvol(ax,gradDescentEvol):\n",
    "    ax.plot(np.arange(gradDescentEvol.shape[0]),gradDescentEvol.J,label='J(theta)')\n",
    "    ax.legend(fontsize=12)\n",
    "    \n",
    "def plotCostFunctionEvolwTest(ax,gradDescentEvol):\n",
    "    ax.plot(np.arange(gradDescentEvol.shape[0]),gradDescentEvol.Jtrain,label='J(train)')\n",
    "    ax.plot(np.arange(gradDescentEvol.shape[0]),gradDescentEvol.Jtest,label='J(test)')\n",
    "    ax.legend(fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "data = pd.read_csv('data/graphicCards.csv')\n",
    "data = data[['memory (Go)','price (euros)']].dropna()\n",
    "data.columns = ['x1','y']\n",
    "data.x1 = data.x1\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot data\n",
    "fig, ax = plt.subplots(figsize=(16,10))\n",
    "plt.plot(data.x1,data.y,'o', ms=2)\n",
    "plt.xlabel('GPU (Go)',fontsize=15)\n",
    "plt.ylabel('prix (€)',fontsize=15)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Contruire un modéle pour nos données\n",
    "\n",
    "\n",
    "* ### Soit: $x_{1}$ la valeur de GPU de nos $m$ carte graphiques, et $y$ leur prix\n",
    "\n",
    "* ### On cherche à déterminer le modèle pour prédire un prix $\\hat{y}$ à partir $x_{1}$: \n",
    "\n",
    "## $\\hat{y} = h_{\\theta}(x_{1})$\n",
    "\n",
    "\n",
    "* ### On défini le paramètre $\\theta_{1}$ qui va lier $x_{1}$ à $\\hat{y}$: \n",
    "\n",
    "## $ h_{\\theta}(x) = \\theta_{1} x_{1}$\n",
    "\n",
    "\n",
    "* ### Rappel math: fonction linéaire $f(x) = kx$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir notre hypothèse (fonction)\n",
    "\n",
    "def hypothesis(x,theta):\n",
    "    return np.dot(x,theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On génère aléatoirement une valeur de départ pour le paramètre theta1 de notre modèle\n",
    "\n",
    "theta = np.random.rand()\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# On plot les données avec notre hypothèse ...\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,10))\n",
    "plt.plot(data.x1, data.y,'o',label='data',ms=2)\n",
    "plt.plot(getHypothesisForPLot(theta).x,getHypothesisForPLot(theta).y ,'r',label='hypothèse')\n",
    "plt.xlabel('GPU (Go)',fontsize=15)\n",
    "plt.ylabel('prix (€)',fontsize=15)\n",
    "plt.title(\"C'est pas ça ....\",fontsize=15)\n",
    "plt.legend(fontsize=15)\n",
    "plt.show();\n",
    "\n",
    "print(\"theta = %f\" % theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Tester la pertinence de notre modèle: la fonction de coût\n",
    "\n",
    "* ### $J(\\theta)$: Véracité de notre modèle\n",
    "\n",
    "* ### Somme Quadratique des erreurs: \n",
    "\n",
    "## $J(\\theta) = \\frac{1}{2m} \\displaystyle\\sum_{i=0}^{m}(\\hat{y}^{(i)} - y^{(i)})^{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On définit notre fonction de coût: somme quadratique (eg: on somme les carré)\n",
    "\n",
    "def costFunction(y,yhat):\n",
    "    return np.square(yhat - y).sum()/(2*y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prix prédis par notre modèle (avec un theta choisi pour illustrer) pour chaque exemple\n",
    "\n",
    "theta = 80\n",
    "costFctData = data.iloc[np.random.randint(0,data.shape[0],10)]\n",
    "costFctData.index = np.arange(10)\n",
    "yhat = hypothesis(costFctData.x1,theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comment fonctionne la fonction de coût: on somme le carré de toute les barre noire\n",
    "\n",
    "fit, ax = plt.subplots(figsize=(16,10))\n",
    "plt.plot(costFctData.x1,costFctData.y,'o',label='data')\n",
    "plt.plot(getHypothesisForPLot(theta).x,getHypothesisForPLot(theta).y,'r',label='hypothèse')\n",
    "for i in range(costFctData.shape[0]):\n",
    "    plt.plot((costFctData.x1[i],costFctData.x1[i]), (min(costFctData.y[i],yhat[i]),max(costFctData.y[i],yhat[i])), 'k-')\n",
    "plt.xlabel('GPU (Go)',fontsize=15)\n",
    "plt.ylabel('prix (€)',fontsize=15)\n",
    "plt.legend(fontsize=15)\n",
    "plt.show();\n",
    "\n",
    "print(\"theta = %f\" % theta)\n",
    "print(\"J(theta) = %f\" % costFunction(costFctData.y,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) À quoi ressemble J(theta) en fonction de theta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Calculons (brutalement) la valeur de J(theta) dans un intervale de valeur de theta1 \n",
    "#     pour observer la forme de notre fonction de coût que nous allons chercher à minimiser\n",
    "\n",
    "thetaRange = np.arange(80-100,80+100,1)\n",
    "costFctEvol = pd.DataFrame({'theta':thetaRange,\n",
    "                            'cost':[costFunction(data.y,hypothesis(data.x1,theta)) \n",
    "                                    for theta in thetaRange]})\n",
    "\n",
    "fit, ax = plt.subplots(figsize=(16,10))\n",
    "plt.plot(costFctEvol.theta,costFctEvol.cost)\n",
    "plt.xlabel('theta',fontSize=15)\n",
    "plt.ylabel('J(theta)',fontSize=15)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) La descente de Gradient\n",
    "\n",
    "* ## On utilise la dérivée de la fonction de coût: $\\frac{d}{d\\theta_{1}}J(\\theta)$\n",
    "\n",
    "    * ### Si $J(\\theta)$ est croissant: $\\frac{d}{d\\theta_{1}}J(\\theta) > 0$\n",
    "    * ### Si $J(\\theta)$ est décroissant: $\\frac{d}{d\\theta_{1}}J(\\theta) < 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La descente de gradient utilise la notion de dérivée, \n",
    "#      illustrée ici avec la fonction carré (qui doit nous en rappeler une autre!)\n",
    "\n",
    "def fct(x):\n",
    "    return np.power(x,2)\n",
    "\n",
    "def fctDeriv(x):\n",
    "    return 2*x\n",
    "\n",
    "fctCarre = pd.DataFrame({'x':np.arange(-10,10,0.1),'y':[fct(x) for x in np.arange(-10,10,0.1)]})\n",
    "fctCarreD = pd.DataFrame({'x':np.arange(-10,10,0.1),\n",
    "                          'y':[fctDeriv(x) for x in np.arange(-10,10,0.1)]})\n",
    "fit, ax = plt.subplots(figsize=(16,10))\n",
    "plt.plot(fctCarre.x,fctCarre.y,label='f(x)')\n",
    "plt.plot(fctCarreD.x,fctCarreD.y,label=\"f'(x)\")\n",
    "plt.grid(True)\n",
    "plt.legend(fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dérivée de la somme quadratique des erreurs:\n",
    "\n",
    "\n",
    "* ### $J(\\theta) = \\frac{1}{2m} \\displaystyle\\sum_{i=0}^{m}(\\hat{y}^{(i)} - y^{(i)})^{2} = \\frac{1}{2m} \\displaystyle\\sum_{i=0}^{m}(\\theta_{1}x_{1}^{(i)} - y^{(i)})^{2}$\n",
    "* ### $\\frac{d}{d\\theta_{1}}J(\\theta) = \\frac{1}{m}\\displaystyle\\sum_{i=0}^{m}(\\hat{y}^{(i)} - y^{(i)}) x_{1}^{(i)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La descente de gradient utilise la dérivée de la fonction de coût \n",
    "#    par rapport au paramètre theta1\n",
    "\n",
    "def costFctDeriv(x,y,yhat):\n",
    "    return ((yhat - y)*x.T).sum(axis=1)/y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithme de la descente de gradient:\n",
    "\n",
    "### $\\begin{matrix} \\text{Répéter jusqu'à convergence:} & \\{ & \\\\ & & \\theta_{1} := \\theta_{1} - \\alpha \\frac{d}{d\\theta_{1}}J(\\theta) \\\\ & \\} & \\end{matrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# À chaque étape de la descente de gradient (jusqu'à la convergence), \n",
    "#   on incremente la valeur de theta1 par ce résultat.\n",
    "#   Alpha est le learning rate\n",
    "\n",
    "def gradDescent(x,y,yhat,alpha):\n",
    "    return -alpha*costFctDeriv(x,y,yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparation du dataset:\n",
    "\n",
    "* ### On sépare aléatoirement les données en 2 (3) échantillons:\n",
    "    * Entraînement / (Validation) / Test\n",
    "    * 80 / 20 (70 / 30) ou 60 / 20 / 20\n",
    "\n",
    "* ### Entraînement: utilisé pour la descente de gradient\n",
    "* ### Validation: utilisé pour l'hyperparamètrage de l'algo\n",
    "* ### Test: utilisé pour mesurer la performance du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train/test\n",
    "index = data.index.values.copy()\n",
    "random.shuffle(index)\n",
    "\n",
    "X_train = data.x1.loc[index[:int(len(index)*0.7)]]\n",
    "X_test = data.x1.loc[index[int(len(index)*0.7):]]\n",
    "Y_train = data.y.loc[index[:int(len(index)*0.7)]]\n",
    "Y_test = data.y.loc[index[int(len(index)*0.7):]]\n",
    "\n",
    "X_train.index = np.arange(X_train.shape[0])\n",
    "Y_train.index = np.arange(Y_train.shape[0])\n",
    "X_test.index = np.arange(X_test.shape[0])\n",
    "Y_test.index = np.arange(Y_test.shape[0])\n",
    "\n",
    "print(\"Train set X shape: {}\".format(X_train.shape))\n",
    "print(\"Train set Y shape: {}\".format(Y_train.shape))\n",
    "print(\"Test set X shape: {}\".format(X_test.shape))\n",
    "print(\"Test set X shape: {}\".format(Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hand-tunning (hyperparamètrage)\n",
    "\n",
    "* ### Learning rate: $\\alpha$ = 0.045 (pour la démo), plus efficace avec $\\alpha$ = 0.03\n",
    "* ### Précision (stop l'apprentissage): $\\epsilon$ = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On utilise donc une valeur de départ pour theta généré aléatoirement entre 0 et 1, \n",
    "#    la valeur du learning rate est fixé à 0.00000003\n",
    "# Epsilon correspond à la précision que l'on veut atteindre pour stopper la descente de gradient\n",
    "\n",
    "thetaInit = np.random.rand()\n",
    "yhat = hypothesis(X_train,thetaInit)\n",
    "alpha = 0.045\n",
    "epsilon = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On prepare un dataframe pour stocker les valeurs de J(theta) et theta1\n",
    "\n",
    "gradDescentEvol = pd.DataFrame({'theta':thetaInit,\n",
    "                                'J':costFunction(Y_train,yhat)},index = np.arange(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert X_train into dataframe\n",
    "X_train = pd.DataFrame({\"x1\": X_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On parametrise deux trois trucs\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [16, 5]\n",
    "costFct = 0\n",
    "count = 0\n",
    "theta = thetaInit\n",
    "\n",
    "# Et on se lance dans la boucle: La descente de gradient!\n",
    "while np.abs(costFunction(Y_train,yhat) - costFct) >= epsilon*costFct:\n",
    "    count += 1\n",
    "    costFct = costFunction(Y_train,yhat)\n",
    "    theta += gradDescent(X_train,Y_train,yhat,alpha)\n",
    "    yhat = hypothesis(X_train,theta)\n",
    "    gradDescentEvol = gradDescentEvol.append(pd.DataFrame({'theta':theta['x1'],\n",
    "                                                           'J':costFunction(Y_train,yhat)},\n",
    "                                                          index = np.arange(1)),\n",
    "                                             ignore_index=True)\n",
    "    fig, ax = plt.subplots(ncols=3)\n",
    "    plotData(ax[0],X_train['x1'],Y_train,theta['x1'],yhat,gradDescentEvol)\n",
    "    plotCostFunction(ax[1],X_train['x1'],Y_train,theta['x1'],gradDescentEvol,thetaInit)\n",
    "    plotCostFunctionEvol(ax[2],gradDescentEvol)\n",
    "    clear_output(wait=True)\n",
    "    display(plt.gcf())\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Prédictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errorFct(y,yhat):\n",
    "    return np.abs(yhat - y).sum()/(y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les résultats:\n",
    "print('La descente de gradient a été réalisé en %i étapes.' % count)\n",
    "print('theta = %f' % theta)\n",
    "print(\"\\nTrain set:\")\n",
    "print('J(theta) = %f' % costFunction(Y_train,hypothesis(X_train['x1'],theta['x1'])))\n",
    "print('Error = %f' % errorFct(Y_train, hypothesis(X_train,theta)))\n",
    "print(\"\\nTest set:\")\n",
    "print('J(theta) = %f' % costFunction(Y_test,hypothesis(X_test,theta['x1'])))\n",
    "print('Error = %f' % errorFct(Y_test, hypothesis(X_test,theta['x1'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faisons une prédiction ....\n",
    "\n",
    "newGPUs = [5,10,14]\n",
    "for newGPU in newGPUs:\n",
    "    print(\"Notre nouvelle carte de %i Go de GPU pourra se vendre autour de %.2f €\" % \n",
    "          (newGPU,newGPU*theta))\n",
    "    \n",
    "plt.rcParams['figure.figsize'] = [14, 8]\n",
    "plt.plot(X_train,Y_train,'o',label='data')\n",
    "plt.plot(getHypothesisForPLot(theta).x,getHypothesisForPLot(theta).y,'r',label='hypothèse')\n",
    "for i in range(X_train.shape[0]):\n",
    "    plt.plot((X_train['x1'][i],X_train['x1'][i]), (min(Y_train[i],yhat[i]),max(Y_train[i],yhat[i])), 'k-')\n",
    "plt.plot(newGPUs,[newGPU*theta for newGPU in newGPUs], 'or', label='prédictions')\n",
    "plt.xlabel('GPU (Go)',fontsize=15)\n",
    "plt.ylabel('prix (€)',fontsize=15)\n",
    "plt.legend(fontsize=15)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7) Régression linéaire multivariable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get data (again...)\n",
    "data = pd.read_csv('data/graphicCards.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration: Deal with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing variables (NaN)\n",
    "print(\"There are empty columns? {}\".format(data.isnull().all().any()))\n",
    "print(\"There are columns with mising values? {}\".format(data.isnull().any().any()))\n",
    "print(\"\\nWhich ones?:\\n\")\n",
    "print(data[data.columns[data.isnull().any()].values].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take look at the chipset feature\n",
    "print(\"Unique values: {}\\n\".format(data.chipset.unique()))\n",
    "print(\"Count values: \\n\")\n",
    "print(data.chipset.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the nan chipset entry?\n",
    "data[data.chipset.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What can we do? We drop it here (not interesting for our purpose). Don't forget to update indexing\n",
    "print(\"Data shape before droping: {}\".format(data.shape))\n",
    "data = data[data.chipset.notnull()]\n",
    "data.index = np.arange(data.shape[0])\n",
    "print(\"Data shape after droping: {}\".format(data.shape))\n",
    "print(\"\\nWhat are columns with missing values now?:\\n\")\n",
    "print(data[data.columns[data.isnull().any()].values].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BoostFreq features is NaN when the constructor don't provide information.\n",
    "# One can seen that as the given graphic card has not boosting option\n",
    "# Solution is to set this value to frequency value if NaN (then we'll deal with NaN frequency entries)\n",
    "data.loc[data['boostFreq (MHz)'].isnull(),'boostFreq (MHz)'] = data.loc[data['boostFreq (MHz)'].isnull(),\n",
    "                                                                        'frequency (MHz)']\n",
    "print(\"\\nWhat are columns with missing values now?:\\n\")\n",
    "print(data[data.columns[data.isnull().any()].values].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about the multi GPU?\n",
    "print(\"Unique values: {}\\n\".format(data['multi GPU'].unique()))\n",
    "print(\"Count values: \\n\")\n",
    "print(data['multi GPU'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLI is the nvidia multi GPU technology while CrossFireX is the AMD one\n",
    "# As distinction beetween AMD and Nvidia is given with chipset features, \n",
    "#    one could transform multi GPU feature to a boolean one:\n",
    "#       - True if card has multi GPU technology (SLI or CrossFireX)\n",
    "#       - False if not (NaN values)\n",
    "data['multi GPU'] = data['multi GPU'].notnull()\n",
    "print(\"multi GPU feature exemples: {}\\n\".format(data['multi GPU'].values[:10]))\n",
    "print(\"What are columns with missing values now?:\\n\")\n",
    "print(data[data.columns[data.isnull().any()].values].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, what about the last NaN values?\n",
    "# Technicaly, frequency and boostFreq NaN corresponds to the same entries\n",
    "# Print all NaN entries\n",
    "data[data.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are several way to deal with that:\n",
    "#       -> find information from other source (constructor site)\n",
    "#       -> estimate missing values from data (could introduce biais!)\n",
    "#       -> drop them (only if there represent a small part of data)\n",
    "#\n",
    "# Here we'll drop them for simplicity\n",
    "print(\"We'll drop {} entries ({}%)\".format(data[data.isnull().any(axis=1)].shape[0],\n",
    "                                           (data[data.isnull().any(axis=1)].shape[0]/data.shape[0])*100))\n",
    "print(\"\\nData shape before droping: {}\".format(data.shape))\n",
    "data = data[data.notnull().all(axis=1)]\n",
    "data.index = np.arange(data.shape[0])\n",
    "print(\"Data shape after droping: {}\\n\".format(data.shape))\n",
    "print(\"There are empty columns? {}\".format(data.isnull().all().any()))\n",
    "print(\"There are columns with mising values? {}\".format(data.isnull().any().any()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration: Features engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let print first rows of our data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to transform our categorial features?\n",
    "# Several things are possible, let's try to transform them as boolean features\n",
    "#  -> First get number of uniques values for each categorials features\n",
    "categorialFeatures = ['builder','chipset','overclok','bus','memory type','direct X (max)']\n",
    "for feature in categorialFeatures:\n",
    "    print(\"\\n{}: ({} unique values)\".format(feature, len(data[feature].unique())))\n",
    "    print(data[feature].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we spot an error in overclok columns: 'no ' with a space. \n",
    "# It seems to be a typo, let replace it by correct 'no' value\n",
    "data.loc[data.overclok == 'no ','overclok'] = 'no'\n",
    " \n",
    "# Other problem: directX == 11.2 -> set it to 11\n",
    "data.loc[data['direct X (max)'] == 11.2,'direct X (max)'] = 11\n",
    "\n",
    "for feature in categorialFeatures:\n",
    "    print(\"\\n{}: ({} unique values)\".format(feature, len(data[feature].unique())))\n",
    "    print(data[feature].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set direct X as string type\n",
    "data['direct X (max)'] = data['direct X (max)'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features with only 2 possible values -> Set booleans\n",
    "data['chipset_amd'] = (data['chipset'] == 'amd') * 1\n",
    "data['overclock'] = (data['overclok'] == 'yes') * 1\n",
    "data['direct_X_11'] = (data['direct X (max)'] == '11.0') * 1\n",
    "\n",
    "data.drop(['chipset', 'overclok', 'direct X (max)'], axis=1, inplace=True)\n",
    "\n",
    "categorialFeatures.remove('chipset')\n",
    "categorialFeatures.remove('overclok')\n",
    "categorialFeatures.remove('direct X (max)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create our boolean features (using get_dummies)\n",
    "data = pd.concat([data.drop(categorialFeatures, axis=1), pd.get_dummies(data[categorialFeatures])], axis=1)    \n",
    "print(\"New data shape: {}\".format(data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now replace boolean values (True, False) by numeric number (1, 0)\n",
    "data[data.columns[data.dtypes == bool]] = data[data.columns[data.dtypes == bool]] * 1\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last feature engineering transformation: replace boostFreq feature by the diff beetween frequency and boostFreq\n",
    "data['boostFreqIncr'] = data['boostFreq (MHz)'] - data['frequency (MHz)']\n",
    "data.drop('boostFreq (MHz)',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last things, make sure index are correct\n",
    "data.index = np.arange(data.shape[0])\n",
    "# That's ok now! Let print new data shape and first rows\n",
    "print(\"Data shape = {}\".format(data.shape))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let store our Y vector (price) in separate Series\n",
    "Y = data['price (euros)']\n",
    "data.drop('price (euros)',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before normalize inputs, let's print a description of non-boolean features\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As boostFreqIncr features has negatives values, in MinMax scaler is not a good thing to do.\n",
    "# Instead we'll use a standard scaler (remove mean, divide by standard deviation)\n",
    "data[data.columns[data.dtypes != object]] = (data[data.columns[data.dtypes != object]] - \\\n",
    "                                             data[data.columns[data.dtypes != object]].mean()) / \\\n",
    "                                            data[data.columns[data.dtypes != object]].std()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add bias: coef directeur -> Une GPU n'est jamais gratuite !\n",
    "data['x0'] = 1\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / test splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to separate our dataset into two sub-sample:\n",
    "#    -> train set: used to train the model (70%)\n",
    "#    -> test set: to measure performances (30%)\n",
    "# Let pick random entries\n",
    "index = data.index.values.copy()\n",
    "random.shuffle(index)\n",
    "\n",
    "X_train = data.loc[index[:int(len(index)*0.7)]]\n",
    "X_test = data.loc[index[int(len(index)*0.7):]]\n",
    "Y_train = Y.loc[index[:int(len(index)*0.7)]]\n",
    "Y_test = Y.loc[index[int(len(index)*0.7):]]\n",
    "\n",
    "print(\"Train set X shape: {}\".format(X_train.shape))\n",
    "print(\"Train set Y shape: {}\".format(Y_train.shape))\n",
    "print(\"Test set X shape: {}\".format(X_test.shape))\n",
    "print(\"Test set X shape: {}\".format(Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariable linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On génère aléatoirement le vecteur des paramétres de notre modèle\n",
    "theta = np.random.rand(X_train.shape[1])-0.5\n",
    "\n",
    "# Compute cost function value for this initialized model\n",
    "yhat = hypothesis(X_train,theta)\n",
    "print(\"Cost Function value = {}\".format(costFunction(Y_train,yhat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now it's time to run the gradient descent, let's parametrize it\n",
    "alpha = 0.03\n",
    "epsilon = 0.01\n",
    "gradDescentEvol = pd.DataFrame()\n",
    "count = 0\n",
    "costFct = 0\n",
    "plot = True\n",
    "\n",
    "# and run it! (it's a quite long...)\n",
    "while np.abs(costFunction(Y_train,yhat) - costFct) >= epsilon * costFct:\n",
    "    count += 1\n",
    "    costFct = costFunction(Y_train,yhat)\n",
    "    theta += gradDescent(X_train,Y_train,yhat,alpha)\n",
    "    yhat = hypothesis(X_train,theta)\n",
    "    gradDescentEvol = gradDescentEvol.append(pd.DataFrame({'Jtrain':costFunction(Y_train,yhat),\n",
    "                                                           'Jtest':costFunction(Y_test,hypothesis(X_test,theta))},\n",
    "                                                          index = np.arange(1)),\n",
    "                                             ignore_index=True)\n",
    "    if plot:\n",
    "        fig, ax = plt.subplots(figsize=(10,5))\n",
    "        plotCostFunctionEvolwTest(ax,gradDescentEvol)\n",
    "        clear_output(wait=True)\n",
    "        display(plt.gcf())\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les résultats:\n",
    "print('La descente de gradient a été réalisé en %i étapes.' % count)\n",
    "print('train_err = %f' % errorFct(Y_train,hypothesis(X_train,theta)))\n",
    "print('test_err = %f' % errorFct(Y_test,hypothesis(X_test,theta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta[[c for c in theta.keys() if 'builder' in c]].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
