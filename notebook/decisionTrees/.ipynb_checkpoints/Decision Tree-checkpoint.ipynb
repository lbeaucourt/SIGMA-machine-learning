{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Titanic dataset\n",
    "data = pd.read_csv(\"data/titanic_dataset.csv\")\n",
    "data.index = data.PassengerId.values\n",
    "data.drop('PassengerId',axis=1,inplace=True)\n",
    "print(\"dataset shape: \" + str(data.shape))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data (Features engineering)\n",
    "\n",
    "# 1) transform string values in int values for categorical features (Sex, Embarked)\n",
    "data['Sex'] = data['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n",
    "data['Embarked'] = data['Embarked'].fillna('U').map( {'S': 0, 'C': 1, 'Q': 2, 'U': 3 } ).astype(int)\n",
    "\n",
    "# 2) Create a new boolean features 'HasCabin' which is False if Cabin is NaN, True otherwise\n",
    "data['HasCabin'] = data.Cabin.notnull() * 1\n",
    "\n",
    "# 3) Drop unnused features\n",
    "data.drop(['Name','Ticket','Cabin'],axis=1,inplace=True)\n",
    "\n",
    "# 4) Missing values: NaN value in Age: drop it for simplicity\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Look the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features and labels into X et Y numpy array\n",
    "X = data.drop('Survived',axis=1).values\n",
    "Y = data.Survived.values.reshape(X.shape[0],1)\n",
    "\n",
    "# Split into train and test set (80/20)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "\n",
    "print(\"Number of entries in the training set : {}\".format(X_train.shape[0]))\n",
    "print(\"Number of entries in the test set     : {}\".format(X_test.shape[0]))\n",
    "print(\"Number of features in the training set: {}\".format(X_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model comparison table\n",
    "compModel = pd.DataFrame({'accuracy':0}, index = ['Decision tree','Random forest','AdaBoost','XGBoost']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DECISION TREE\n",
    "print(\"DECISION TREE:\")\n",
    "\n",
    "# Prepare DecisionTree model and fit it to the data\n",
    "dt = DecisionTreeClassifier().fit(X_train, Y_train)\n",
    "\n",
    "# Make prediction\n",
    "predictions = dt.predict(X_test)\n",
    "print('Prediction exemples: ' + str(dt.predict(X_test[:10])))\n",
    "\n",
    "# Get accuracy of this model\n",
    "score = dt.score(X_test, Y_test)\n",
    "compModel['Decision tree'] = score\n",
    "print(\"Decision Tree accuracy: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM FOREST\n",
    "print(\"RANDOM FOREST:\")\n",
    "\n",
    "# Prepare DecisionTree model and fit it to the data\n",
    "rf = RandomForestClassifier(n_estimators=50).fit(X_train, Y_train.reshape(Y_train.shape[0],))\n",
    "\n",
    "# Make prediction\n",
    "predictions = rf.predict(X_test)\n",
    "print('Prediction exemples: ' + str(rf.predict(X_test[:10])))\n",
    "\n",
    "# Get accuracy of this model\n",
    "score = rf.score(X_test, Y_test)\n",
    "compModel['Random forest'] = score\n",
    "print(\"Random Forest accuracy: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost\n",
    "print(\"AdaBoost:\")\n",
    "\n",
    "# Prepare DecisionTree model and fit it to the data\n",
    "adaB = AdaBoostClassifier(n_estimators=50).fit(X_train, Y_train.reshape(Y_train.shape[0],))\n",
    "\n",
    "# Make prediction\n",
    "predictions = adaB.predict(X_test)\n",
    "print('Prediction exemples: ' + str(adaB.predict(X_test[:10])))\n",
    "\n",
    "# Get accuracy of this model\n",
    "score = adaB.score(X_test, Y_test)\n",
    "compModel['AdaBoost'] = score\n",
    "print(\"AdaBoost accuracy: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting (using XGBoost)\n",
    "print(\"Gradient boosting\")\n",
    "print()\n",
    "\n",
    "# Prepare dataset\n",
    "xgb_train = xgb.DMatrix(X_train, label = Y_train)\n",
    "xgb_test = xgb.DMatrix(X_test, label = Y_test)\n",
    "watchlist = [(xgb_train, 'train'), (xgb_test, 'valid')]\n",
    "\n",
    "# Prepare model (hyperparameters)\n",
    "xgb_pars = {'min_child_weight': 5, 'eta': 0.9, 'max_depth': 15, 'gamma': 0.5, \n",
    "            'booster' : 'gbtree', 'objective': 'binary:logistic'}\n",
    "\n",
    "xgb_pars = {'min_child_weight': 5, 'eta': 0.9, 'max_depth': 1, 'gamma': 0.3, \n",
    "            'booster' : 'gbtree', 'objective': 'binary:logistic'}\n",
    "\n",
    "# Train the XGBoost model\n",
    "xgbModel = xgb.train(xgb_pars, xgb_train, 50, watchlist, early_stopping_rounds=50, maximize=False, verbose_eval=10)\n",
    "print('Modeling RMSLE %.5f' % xgbModel.best_score)\n",
    "print()\n",
    "\n",
    "# Make prediction\n",
    "predictions = (xgbModel.predict(xgb_test) > 0.5) * 1\n",
    "print('Prediction exemples: ' + str((xgbModel.predict(xgb_test)[:10] > 0.5)*1))\n",
    "\n",
    "# Get accuracy of this model\n",
    "score = (Y_test.reshape(Y_test.shape[0],) == predictions).sum() / Y_test.shape[0]\n",
    "compModel['Gradient boosting'] = score\n",
    "print(\"Gradient boosting accuracy: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performances:\n",
    "compModel.T.sort_values('accuracy',ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
